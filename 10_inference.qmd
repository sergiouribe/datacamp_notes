---
title: "Inference"
format: html
editor: visual
---

```{r}
pacman::p_load(tidyverse)
```

![![](images/clipboard-3361331156.png)](images/clipboard-3096119592.png)

![![](images/clipboard-1328109182.png)](images/clipboard-3821990180.png)

he importance of hypothesis testing and its application in real-world scenarios. Hypothesis testing helps determine if the observed differences in data are meaningful or due to random chance. Here are the key points you covered:

-   **A/B Testing**: You explored how Electronic Arts used A/B testing to compare two versions of a web page to see which one resulted in higher pre-orders for SimCity 5. This method helps in making data-driven decisions by comparing outcomes of different scenarios.

-   **Sample Mean Calculation**: You learned to calculate the sample mean, which is a type of point estimate. For example, you calculated the mean annual compensation of data scientists from a survey dataset.

-   **Bootstrap Distribution**: You generated a bootstrap distribution of sample means by resampling the dataset multiple times. This distribution helps estimate the standard error of the sample statistic.

-   **Z-Scores**: You standardized values using z-scores, which indicate how many standard deviations a sample statistic is from the hypothesized parameter value. For instance, you calculated the z-score for the proportion of late shipments.

Here's an example of calculating a z-score:

```         
# Hypothesize that the proportion is 6%
late_prop_hyp <- 0.06

# Calculate the standard error
std_error <- late_shipments_boot_distn %>% 
  summarize(sd_late_prop = sd(late_prop)) %>% 
  pull(sd_late_prop)

# Find z-score of late_prop_samp
z_score <- (late_prop_samp - late_prop_hyp) / std_error

# See the results
z_score
```

![](images/clipboard-1430535133.png)

![](images/clipboard-2376425064.png)

-   **p-values**: These quantify the evidence against the null hypothesis. A small p-value suggests rejecting the null hypothesis in favor of the alternative hypothesis, while a large p-value suggests sticking with the null hypothesis.

-   **Significance Level (α)**: This is the cutoff point for deciding whether a p-value is small or large. Common choices are 0.05, 0.10, and 0.01. If the p-value is less than or equal to α, you reject the null hypothesis.

-   **Confidence Intervals**: These provide a range of plausible values for a population parameter. For a significance level of 0.05, a 95% confidence interval is used.

-   **Type I and Type II Errors**: Type I errors (false positives) occur when the null hypothesis is incorrectly rejected. Type II errors (false negatives) occur when the null hypothesis is incorrectly accepted.

![](images/clipboard-3386067985.png)

## t-test

```{r}
# Calculate the numerator of the test statistic
numerator <- (xbar_no - xbar_yes)

# Calculate the denominator of the test statistic
denominator <- sqrt( (s_no ^ 2 / n_no) + (s_yes^ 2 / n_yes))

# Calculate the test statistic
t_stat <- numerator / denominator

# See the result
t_stat
```

![![](images/clipboard-2487474175.png)](images/clipboard-3283527699.png)

## t-test paired

![](images/clipboard-863783114.png)

![](images/clipboard-1761918466.png)

![](images/clipboard-2344372318.png)

## Two samples and ANOVA

-   **ANOVA Tests**: You explored how ANOVA (Analysis of Variance) tests can be used to check if there are differences in means across more than two groups. For example, to test if mean annual compensation differs by job satisfaction levels, you fit a linear regression model and then performed an ANOVA test.

    -   **Code Example**:

        ```         
        model <- lm(compensation ~ job_satisfaction, data = survey_data)
        anova_result <- anova(model)
        ```

    -   The p-value in the ANOVA result indicates if there are significant differences between the groups.

-   **Visualizing Data**: Before conducting tests, you visualized the distributions of the numeric variable using box plots. This helps in understanding the data better.

    -   **Code Example**:

        ```         
        ggplot(late_shipments, aes(x = shipment_mode, y = pack_price)) +
          geom_boxplot() +
          coord_flip()
        ```

-   **Pairwise t-tests**: Since ANOVA doesn't specify which groups differ, you used pairwise t-tests to compare each pair of categories. This involves running multiple tests and adjusting p-values to control for false positives.

    -   **Code Example**:

        ```         
        pairwise.t.test(late_shipments$pack_price, late_shipments$shipment_mode, p.adjust.method = "holm")
        ```

-   **P-value Adjustments**: You learned about different methods to adjust p-values, like the Bonferroni and Holm adjustments, to reduce the risk of false positives when conducting multiple tests.

## RECAP

![](images/clipboard-836899424.png)

-   **Population Proportion (p)**: This is the unknown parameter we are trying to estimate.

-   **Sample Proportion (p-hat)**: This is the proportion observed in your sample.

-   **Hypothesized Proportion (p-zero)**: This is the value you are testing against.

You also learned how to calculate the standardized test statistic (z-score) for one-sample proportion tests without using bootstrap distributions:

-   **Z-score Calculation**:

    -   Formula: ( z = \\dfrac{\\hat{p} - p\_{0}}{\\sqrt{\\dfrac{p\_{0} \\cdot (1 - p\_{0})}{n}}} )

    -   This uses the sample proportion (p-hat), hypothesized proportion (p-zero), and sample size (n).

-   **P-value Calculation**:

    -   For left-tailed tests, use `pnorm(z, lower.tail = TRUE)`.

    -   For right-tailed tests, use `pnorm(z, lower.tail = FALSE)`.

    -   For two-tailed tests, double the left-tailed p-value.

You explored why a z-test is used for proportions instead of a t-test, noting that proportions only require one estimate, reducing uncertainty.

Here’s an example code snippet for calculating the z-score:

```         
p_hat <- 0.55  # Sample proportion
p_zero <- 0.50  # Hypothesized proportion
n <- 100  # Sample size

z <- (p_hat - p_zero) / sqrt((p_zero * (1 - p_zero)) / n)
```

This calculation helps you determine if the observed proportion significantly differs from the hypothesized value.

## Proportion test

![](images/clipboard-3322735916.png)
